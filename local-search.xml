<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>A unified perspective of RLHF</title>
    <link href="/2024/10/29/Work/A-unified-perspective-of-RLHF/"/>
    <url>/2024/10/29/Work/A-unified-perspective-of-RLHF/</url>
    
    <content type="html"><![CDATA[<h1>RLHF</h1><h2 id="PPO">PPO</h2><blockquote><p><a href="https://arxiv.org/pdf/1707.06347">Proximal Policy Optimization Algorithms</a></p><p><a href="https://arxiv.org/pdf/2305.18290">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a></p><p><a href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/">https://lilianweng.github.io/posts/2018-04-08-policy-gradient/</a></p><p><a href="https://zhuanlan.zhihu.com/p/1686790674">关于LLM+RL(HF)的片面脉络梳理</a></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Wrok</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM RLHF</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
