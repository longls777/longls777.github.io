<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>A unified perspective of RLHF</title>
    <link href="/2024/10/29/Work/A-unified-perspective-of-RLHF/"/>
    <url>/2024/10/29/Work/A-unified-perspective-of-RLHF/</url>
    
    <content type="html"><![CDATA[<h1>Currently popular RLHF Method</h1><p><img src="https://longls777.oss-cn-beijing.aliyuncs.com/img/image-20241030133028665.png" alt="rlhf pipline"></p><h2 id="PPO">PPO</h2><h2 id="DPO">DPO</h2><h2 id="ORPO">ORPO</h2><h2 id="KTO">KTO</h2><h2 id="IPO">IPO</h2><h2 id="SimPO">SimPO</h2><h1>Relate Work</h1><h2 id="UNA-Unifying-Alignments-of-RLHF-PPO-DPO-and-KTO-by-a-Generalized-Implicit-Reward-Function">UNA: Unifying Alignments of RLHF/PPO, DPO and KTO by a Generalized Implicit Reward Function</h2><p><strong>paper</strong>: <a href="https://arxiv.org/pdf/2408.15339">https://arxiv.org/pdf/2408.15339</a></p><p><strong>source</strong>: Salesforce</p><blockquote><p><a href="https://arxiv.org/pdf/1707.06347">Proximal Policy Optimization Algorithms</a></p><p><a href="https://arxiv.org/pdf/2305.18290">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a></p><p><a href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/">https://lilianweng.github.io/posts/2018-04-08-policy-gradient/</a></p><p><a href="https://zhuanlan.zhihu.com/p/1686790674">关于LLM+RL(HF)的片面脉络梳理</a></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM RLHF</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
